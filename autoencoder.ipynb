{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "272f90a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from plot import color, semantic_embeddings_plot\n",
    "from dataset import StreetHazardDataset, PadToMultipleOf16, StreetHazardDatasetTriplet\n",
    "from _model import DINOv2_SemanticSegmentation\n",
    "from loss import nt_xent_loss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "COLAB = False\n",
    "KAGGLE = False\n",
    "NUM_CLASSES = 13\n",
    "SEED = 42\n",
    "\n",
    "# Seed everything\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_IMAGE_SIZE = (720, 1280)\n",
    "IMAGE_SIZE = PadToMultipleOf16().convert_dims((500,500))\n",
    "# assert IMAGE_SIZE[0] %4 == 0 and IMAGE_SIZE[1] % 4 == 0\n",
    "BATCH_SIZE = 8\n",
    "POSITIVE_PAIRS = True\n",
    "PIXEL_PER_CLASS = 3\n",
    "\n",
    "# Percorsi dei file\n",
    "if COLAB:\n",
    "    annotations_train_file = \"/content/train/train.odgt\"\n",
    "    annotation_val_file = \"/content/train/validation.odgt\"\n",
    "    img_dir = \"/content/train\"\n",
    "elif KAGGLE:\n",
    "    annotations_train_file = \"../input/streethazards-train/train/train.odgt\"\n",
    "    annotation_val_file = \"../input/streethazards-train/train/validation.odgt\"\n",
    "    img_dir = \"../input/streethazards-train/train\"\n",
    "else:\n",
    "    annotations_train_file = \"/home/federico/.cache/kagglehub/datasets/lucadome/streethazards-train/versions/1/train/train.odgt\"\n",
    "    annotation_val_file = \"/home/federico/.cache/kagglehub/datasets/lucadome/streethazards-train/versions/1/train/validation.odgt\"\n",
    "    annotation_test_file = \"/home/federico/Downloads/streethazards_test/test/test.odgt\"\n",
    "    img_dir = \"/home/federico/.cache/kagglehub/datasets/lucadome/streethazards-train/versions/1/train/\"\n",
    "    img_dir_test = \"/home/federico/Downloads/streethazards_test/test/\"\n",
    "\n",
    "\n",
    "image_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((352, 640), interpolation=InterpolationMode.BILINEAR),\n",
    "        # transforms.RandomCrop(512),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "target_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((352, 640), interpolation=InterpolationMode.NEAREST),\n",
    "        # transforms.RandomCrop(512),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Crea il dataset\n",
    "dataset_train = StreetHazardDataset(\n",
    "    annotations_train_file,\n",
    "    img_dir,\n",
    "    image_transform=image_transform,\n",
    "    target_transform=target_transform,\n",
    "    positive_pairs=True,\n",
    "    pixel_per_class=PIXEL_PER_CLASS\n",
    ")\n",
    "\n",
    "dataset_val = StreetHazardDataset(\n",
    "    annotation_val_file,\n",
    "    img_dir,\n",
    "    image_transform=image_transform,\n",
    "    target_transform=target_transform,\n",
    "    positive_pairs=False,\n",
    "    pixel_per_class=PIXEL_PER_CLASS\n",
    ")\n",
    "\n",
    "dataset_test = StreetHazardDataset(\n",
    "    annotation_test_file,\n",
    "    img_dir_test,\n",
    "    image_transform=image_transform,\n",
    "    target_transform=target_transform,\n",
    "    positive_pairs=False,\n",
    "    pixel_per_class=PIXEL_PER_CLASS\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dl_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "dl_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f349dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# --- Encoder (ResNet-18 backbone without avgpool & fc) ---\n",
    "class ResNet18Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        backbone = resnet18(weights=None)  # untrained ResNet18\n",
    "\n",
    "        # Take all layers except avgpool and fc\n",
    "        self.features = nn.Sequential(*list(backbone.children())[:-2])  # Output (B, 512, H/32, W/32)\n",
    "\n",
    "        # self.fc = nn.Linear(512 * \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# --- Decoder ---\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, output_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decode_blocks = nn.Sequential(\n",
    "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(256),\n",
    "            # nn.ReLU(True),\n",
    "\n",
    "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(128),\n",
    "            # nn.ReLU(True),\n",
    "\n",
    "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            # nn.ReLU(True),\n",
    "\n",
    "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            # nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(32),\n",
    "            # nn.ReLU(True),\n",
    "\n",
    "            # # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            # nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            # nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(16),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.BatchNorm2d(16),\n",
    "            # nn.ReLU(True),\n",
    "\n",
    "            # nn.Conv2d(16, output_channels, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.Sigmoid() \n",
    "\n",
    "            # nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.Conv2d(32, output_channels, kernel_size=3, stride=1, padding=1),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode_blocks(x)\n",
    "\n",
    "# --- Full Autoencoder ---\n",
    "class ResNet18Autoencoder(nn.Module):\n",
    "    def __init__(self, output_channels=3, bottleneck_features=512):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNet18Encoder()\n",
    "        self.decoder = SimpleDecoder(output_channels)\n",
    "        self.fc1 = nn.Linear(512 * 11 * 20, bottleneck_features)\n",
    "        self.fc2 = nn.Linear(bottleneck_features, 512 * 11 * 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        z = self.fc1(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.fc2(z)\n",
    "        z = z.view(z.size(0), 512, 11, 20)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "826d065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 by Gongfan Fang, Zhejiang University.\n",
    "# All rights reserved.\n",
    "import warnings\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def _fspecial_gauss_1d(size: int, sigma: float) -> Tensor:\n",
    "    r\"\"\"Create 1-D gauss kernel\n",
    "    Args:\n",
    "        size (int): the size of gauss kernel\n",
    "        sigma (float): sigma of normal distribution\n",
    "    Returns:\n",
    "        torch.Tensor: 1D kernel (1 x 1 x size)\n",
    "    \"\"\"\n",
    "    coords = torch.arange(size, dtype=torch.float)\n",
    "    coords -= size // 2\n",
    "\n",
    "    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n",
    "    g /= g.sum()\n",
    "\n",
    "    return g.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def gaussian_filter(input: Tensor, win: Tensor) -> Tensor:\n",
    "    r\"\"\" Blur input with 1-D kernel\n",
    "    Args:\n",
    "        input (torch.Tensor): a batch of tensors to be blurred\n",
    "        window (torch.Tensor): 1-D gauss kernel\n",
    "    Returns:\n",
    "        torch.Tensor: blurred tensors\n",
    "    \"\"\"\n",
    "    assert all([ws == 1 for ws in win.shape[1:-1]]), win.shape\n",
    "    if len(input.shape) == 4:\n",
    "        conv = F.conv2d\n",
    "    elif len(input.shape) == 5:\n",
    "        conv = F.conv3d\n",
    "    else:\n",
    "        raise NotImplementedError(input.shape)\n",
    "\n",
    "    C = input.shape[1]\n",
    "    out = input\n",
    "    for i, s in enumerate(input.shape[2:]):\n",
    "        if s >= win.shape[-1]:\n",
    "            out = conv(out, weight=win.transpose(2 + i, -1), stride=1, padding=0, groups=C)\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"Skipping Gaussian Smoothing at dimension 2+{i} for input: {input.shape} and win size: {win.shape[-1]}\"\n",
    "            )\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def _ssim(\n",
    "    X: Tensor,\n",
    "    Y: Tensor,\n",
    "    data_range: float,\n",
    "    win: Tensor,\n",
    "    size_average: bool = True,\n",
    "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03)\n",
    ") -> Tuple[Tensor, Tensor]:\n",
    "    r\"\"\" Calculate ssim index for X and Y\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): images\n",
    "        Y (torch.Tensor): images\n",
    "        data_range (float or int): value range of input images. (usually 1.0 or 255)\n",
    "        win (torch.Tensor): 1-D gauss kernel\n",
    "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: ssim results.\n",
    "    \"\"\"\n",
    "    K1, K2 = K\n",
    "    # batch, channel, [depth,] height, width = X.shape\n",
    "    compensation = 1.0\n",
    "\n",
    "    C1 = (K1 * data_range) ** 2\n",
    "    C2 = (K2 * data_range) ** 2\n",
    "\n",
    "    win = win.to(X.device, dtype=X.dtype)\n",
    "\n",
    "    mu1 = gaussian_filter(X, win)\n",
    "    mu2 = gaussian_filter(Y, win)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = compensation * (gaussian_filter(X * X, win) - mu1_sq)\n",
    "    sigma2_sq = compensation * (gaussian_filter(Y * Y, win) - mu2_sq)\n",
    "    sigma12 = compensation * (gaussian_filter(X * Y, win) - mu1_mu2)\n",
    "\n",
    "    cs_map = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)  # set alpha=beta=gamma=1\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)) * cs_map\n",
    "\n",
    "    ssim_per_channel = torch.flatten(ssim_map, 2).mean(-1)\n",
    "    cs = torch.flatten(cs_map, 2).mean(-1)\n",
    "    return ssim_per_channel, cs\n",
    "\n",
    "\n",
    "def ssim(\n",
    "    X: Tensor,\n",
    "    Y: Tensor,\n",
    "    data_range: float = 255,\n",
    "    size_average: bool = True,\n",
    "    win_size: int = 11,\n",
    "    win_sigma: float = 1.5,\n",
    "    win: Optional[Tensor] = None,\n",
    "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
    "    nonnegative_ssim: bool = False,\n",
    ") -> Tensor:\n",
    "    r\"\"\" interface of ssim\n",
    "    Args:\n",
    "        X (torch.Tensor): a batch of images, (N,C,H,W)\n",
    "        Y (torch.Tensor): a batch of images, (N,C,H,W)\n",
    "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "        win_size: (int, optional): the size of gauss kernel\n",
    "        win_sigma: (float, optional): sigma of normal distribution\n",
    "        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n",
    "        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: ssim results\n",
    "    \"\"\"\n",
    "    if not X.shape == Y.shape:\n",
    "        raise ValueError(f\"Input images should have the same dimensions, but got {X.shape} and {Y.shape}.\")\n",
    "\n",
    "    for d in range(len(X.shape) - 1, 1, -1):\n",
    "        X = X.squeeze(dim=d)\n",
    "        Y = Y.squeeze(dim=d)\n",
    "\n",
    "    if len(X.shape) not in (4, 5):\n",
    "        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n",
    "\n",
    "    #if not X.type() == Y.type():\n",
    "    #    raise ValueError(f\"Input images should have the same dtype, but got {X.type()} and {Y.type()}.\")\n",
    "\n",
    "    if win is not None:  # set win_size\n",
    "        win_size = win.shape[-1]\n",
    "\n",
    "    if not (win_size % 2 == 1):\n",
    "        raise ValueError(\"Window size should be odd.\")\n",
    "\n",
    "    if win is None:\n",
    "        win = _fspecial_gauss_1d(win_size, win_sigma)\n",
    "        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n",
    "\n",
    "    ssim_per_channel, cs = _ssim(X, Y, data_range=data_range, win=win, size_average=False, K=K)\n",
    "    if nonnegative_ssim:\n",
    "        ssim_per_channel = torch.relu(ssim_per_channel)\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_per_channel.mean()\n",
    "    else:\n",
    "        return ssim_per_channel.mean(1)\n",
    "\n",
    "\n",
    "def ms_ssim(\n",
    "    X: Tensor,\n",
    "    Y: Tensor,\n",
    "    data_range: float = 255,\n",
    "    size_average: bool = True,\n",
    "    win_size: int = 11,\n",
    "    win_sigma: float = 1.5,\n",
    "    win: Optional[Tensor] = None,\n",
    "    weights: Optional[List[float]] = None,\n",
    "    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03)\n",
    ") -> Tensor:\n",
    "    r\"\"\" interface of ms-ssim\n",
    "    Args:\n",
    "        X (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
    "        Y (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n",
    "        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "        win_size: (int, optional): the size of gauss kernel\n",
    "        win_sigma: (float, optional): sigma of normal distribution\n",
    "        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n",
    "        weights (list, optional): weights for different levels\n",
    "        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "    Returns:\n",
    "        torch.Tensor: ms-ssim results\n",
    "    \"\"\"\n",
    "    if not X.shape == Y.shape:\n",
    "        raise ValueError(f\"Input images should have the same dimensions, but got {X.shape} and {Y.shape}.\")\n",
    "\n",
    "    for d in range(len(X.shape) - 1, 1, -1):\n",
    "        X = X.squeeze(dim=d)\n",
    "        Y = Y.squeeze(dim=d)\n",
    "\n",
    "    #if not X.type() == Y.type():\n",
    "    #    raise ValueError(f\"Input images should have the same dtype, but got {X.type()} and {Y.type()}.\")\n",
    "\n",
    "    if len(X.shape) == 4:\n",
    "        avg_pool = F.avg_pool2d\n",
    "    elif len(X.shape) == 5:\n",
    "        avg_pool = F.avg_pool3d\n",
    "    else:\n",
    "        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n",
    "\n",
    "    if win is not None:  # set win_size\n",
    "        win_size = win.shape[-1]\n",
    "\n",
    "    if not (win_size % 2 == 1):\n",
    "        raise ValueError(\"Window size should be odd.\")\n",
    "\n",
    "    smaller_side = min(X.shape[-2:])\n",
    "    assert smaller_side > (win_size - 1) * (\n",
    "        2 ** 4\n",
    "    ), \"Image size should be larger than %d due to the 4 downsamplings in ms-ssim\" % ((win_size - 1) * (2 ** 4))\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]\n",
    "    weights_tensor = X.new_tensor(weights)\n",
    "\n",
    "    if win is None:\n",
    "        win = _fspecial_gauss_1d(win_size, win_sigma)\n",
    "        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n",
    "\n",
    "    levels = weights_tensor.shape[0]\n",
    "    mcs = []\n",
    "    for i in range(levels):\n",
    "        ssim_per_channel, cs = _ssim(X, Y, win=win, data_range=data_range, size_average=False, K=K)\n",
    "\n",
    "        if i < levels - 1:\n",
    "            mcs.append(torch.relu(cs))\n",
    "            padding = [s % 2 for s in X.shape[2:]]\n",
    "            X = avg_pool(X, kernel_size=2, padding=padding)\n",
    "            Y = avg_pool(Y, kernel_size=2, padding=padding)\n",
    "\n",
    "    ssim_per_channel = torch.relu(ssim_per_channel)  # type: ignore  # (batch, channel)\n",
    "    mcs_and_ssim = torch.stack(mcs + [ssim_per_channel], dim=0)  # (level, batch, channel)\n",
    "    ms_ssim_val = torch.prod(mcs_and_ssim ** weights_tensor.view(-1, 1, 1), dim=0)\n",
    "\n",
    "    if size_average:\n",
    "        return ms_ssim_val.mean()\n",
    "    else:\n",
    "        return ms_ssim_val.mean(1)\n",
    "\n",
    "\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_range: float = 255,\n",
    "        size_average: bool = True,\n",
    "        win_size: int = 11,\n",
    "        win_sigma: float = 1.5,\n",
    "        channel: int = 3,\n",
    "        spatial_dims: int = 2,\n",
    "        K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
    "        nonnegative_ssim: bool = False,\n",
    "    ) -> None:\n",
    "        r\"\"\" class for ssim\n",
    "        Args:\n",
    "            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "            win_size: (int, optional): the size of gauss kernel\n",
    "            win_sigma: (float, optional): sigma of normal distribution\n",
    "            channel (int, optional): input channels (default: 3)\n",
    "            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "            nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n",
    "        \"\"\"\n",
    "\n",
    "        super(SSIM, self).__init__()\n",
    "        self.win_size = win_size\n",
    "        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n",
    "        self.size_average = size_average\n",
    "        self.data_range = data_range\n",
    "        self.K = K\n",
    "        self.nonnegative_ssim = nonnegative_ssim\n",
    "\n",
    "    def forward(self, X: Tensor, Y: Tensor) -> Tensor:\n",
    "        return ssim(\n",
    "            X,\n",
    "            Y,\n",
    "            data_range=self.data_range,\n",
    "            size_average=self.size_average,\n",
    "            win=self.win,\n",
    "            K=self.K,\n",
    "            nonnegative_ssim=self.nonnegative_ssim,\n",
    "        )\n",
    "\n",
    "\n",
    "class MS_SSIM(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_range: float = 255,\n",
    "        size_average: bool = True,\n",
    "        win_size: int = 11,\n",
    "        win_sigma: float = 1.5,\n",
    "        channel: int = 3,\n",
    "        spatial_dims: int = 2,\n",
    "        weights: Optional[List[float]] = None,\n",
    "        K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n",
    "    ) -> None:\n",
    "        r\"\"\" class for ms-ssim\n",
    "        Args:\n",
    "            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n",
    "            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n",
    "            win_size: (int, optional): the size of gauss kernel\n",
    "            win_sigma: (float, optional): sigma of normal distribution\n",
    "            channel (int, optional): input channels (default: 3)\n",
    "            weights (list, optional): weights for different levels\n",
    "            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n",
    "        \"\"\"\n",
    "\n",
    "        super(MS_SSIM, self).__init__()\n",
    "        self.win_size = win_size\n",
    "        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n",
    "        self.size_average = size_average\n",
    "        self.data_range = data_range\n",
    "        self.weights = weights\n",
    "        self.K = K\n",
    "\n",
    "    def forward(self, X: Tensor, Y: Tensor) -> Tensor:\n",
    "        return ms_ssim(\n",
    "            X,\n",
    "            Y,\n",
    "            data_range=self.data_range,\n",
    "            size_average=self.size_average,\n",
    "            win=self.win,\n",
    "            weights=self.weights,\n",
    "            K=self.K,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06ed8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Negative_SSIM(SSIM):\n",
    "    def forward(self, X: Tensor, Y: Tensor) -> Tensor:\n",
    "        \"\"\"Override the forward method to return negative SSIM.\"\"\"\n",
    "        ssim_value = super().forward(X, Y)\n",
    "        return (1 - ssim_value) / 2  # Return negative SSIM\n",
    "\n",
    "ssim_loss = Negative_SSIM(data_range=1.0, size_average=False, channel=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9553f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18Autoencoder(output_channels=3, bottleneck_features=512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eef57b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 641/641 [06:03<00:00,  1.76it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:05<00:00,  2.88it/s]\n",
      "Training: 100%|██████████| 641/641 [05:46<00:00,  1.85it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:06<00:00,  2.82it/s]\n",
      "Training: 100%|██████████| 641/641 [06:07<00:00,  1.74it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:08<00:00,  2.76it/s]\n",
      "Training: 100%|██████████| 641/641 [06:04<00:00,  1.76it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:07<00:00,  2.79it/s]\n",
      "Training: 100%|██████████| 641/641 [06:06<00:00,  1.75it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:09<00:00,  2.71it/s]\n",
      "Training: 100%|██████████| 641/641 [06:07<00:00,  1.74it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:09<00:00,  2.72it/s]\n",
      "Training: 100%|██████████| 641/641 [06:01<00:00,  1.77it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:07<00:00,  2.77it/s]\n",
      "Training: 100%|██████████| 641/641 [06:06<00:00,  1.75it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:08<00:00,  2.74it/s]\n",
      "Training: 100%|██████████| 641/641 [06:03<00:00,  1.76it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:07<00:00,  2.78it/s]\n",
      "Training: 100%|██████████| 641/641 [06:07<00:00,  1.74it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:07<00:00,  2.80it/s]\n",
      "Training: 100%|██████████| 641/641 [05:54<00:00,  1.81it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:06<00:00,  2.81it/s]\n",
      "Training: 100%|██████████| 641/641 [05:58<00:00,  1.79it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:08<00:00,  2.73it/s]\n",
      "Training: 100%|██████████| 641/641 [06:03<00:00,  1.77it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:05<00:00,  2.88it/s]\n",
      "Training: 100%|██████████| 641/641 [05:53<00:00,  1.81it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:06<00:00,  2.85it/s]\n",
      "Training: 100%|██████████| 641/641 [06:08<00:00,  1.74it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:07<00:00,  2.78it/s]\n",
      "Training: 100%|██████████| 641/641 [05:55<00:00,  1.80it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:07<00:00,  2.80it/s]\n",
      "Training: 100%|██████████| 641/641 [05:54<00:00,  1.81it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:07<00:00,  2.79it/s]\n",
      "Training: 100%|██████████| 641/641 [05:54<00:00,  1.81it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:08<00:00,  2.76it/s]\n",
      "Training: 100%|██████████| 641/641 [05:54<00:00,  1.81it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:06<00:00,  2.82it/s]\n",
      "Training: 100%|██████████| 641/641 [05:54<00:00,  1.81it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:08<00:00,  2.74it/s]\n",
      "Training: 100%|██████████| 641/641 [05:55<00:00,  1.80it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:07<00:00,  2.78it/s]\n",
      "Training: 100%|██████████| 641/641 [05:56<00:00,  1.80it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:06<00:00,  2.81it/s]\n",
      "Training: 100%|██████████| 641/641 [05:51<00:00,  1.82it/s]\n",
      "Testing: 100%|██████████| 188/188 [01:10<00:00,  2.68it/s]\n",
      "Training:  47%|████▋     | 301/641 [03:03<03:27,  1.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mssim_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, test_dataloader, optimizer, criterion, device, epochs, logger)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (images, _, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m      9\u001b[0m         step \u001b[38;5;241m=\u001b[39m e \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader) \u001b[38;5;241m+\u001b[39m i\n\u001b[1;32m     11\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/repos/ML4CV_Assignment/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/ML4CV_Assignment/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    739\u001b[0m ):\n",
      "File \u001b[0;32m~/repos/ML4CV_Assignment/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/repos/ML4CV_Assignment/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/repos/ML4CV_Assignment/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/repos/ML4CV_Assignment/dataset.py:42\u001b[0m, in \u001b[0;36mStreetHazardDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     39\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfpath_img\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     40\u001b[0m seg_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfpath_segm\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 42\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(seg_path)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_transform:\n",
      "File \u001b[0;32m~/repos/ML4CV_Assignment/.venv/lib/python3.10/site-packages/PIL/Image.py:993\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    991\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/ML4CV_Assignment/.venv/lib/python3.10/site-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Standard train loop\n",
    "def train(model, dataloader, test_dataloader, optimizer, criterion, device, epochs, logger):\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        for i, (images, _, _) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "            step = e * len(dataloader) + i\n",
    "\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_images = model(images)\n",
    "            loss1 = criterion(recon_images, images).mean()\n",
    "            loss2 = F.mse_loss(recon_images, images)\n",
    "            loss_final = loss1 + loss2\n",
    "            loss_final.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                logger.add_scalar(\"Train/Total Loss\", loss_final.item(), step)\n",
    "                logger.add_scalar(\"Train/SSIM\", loss1.item(), step)\n",
    "                logger.add_scalar(\"Train/MSE\", loss2.item(), step)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                diff = torch.abs(recon_images - images)\n",
    "                grid = torch.cat([images, recon_images, diff], dim=3)\n",
    "                grid = torch.cat([grid[0], grid[1], grid[2], grid[3], grid[4], grid[5], grid[6], grid[7]], dim=1)\n",
    "                logger.add_image(\"Train Images\", grid, step)\n",
    "\n",
    "        model.eval()\n",
    "        for i, (images, _) in enumerate(tqdm(test_dataloader, desc=\"Testing\")):\n",
    "            images = images.to(device)\n",
    "            with torch.no_grad():\n",
    "                recon_images = model(images)\n",
    "\n",
    "            if i == 0:\n",
    "                diff = torch.abs(recon_images - images)\n",
    "                grid = torch.cat([images, recon_images, diff], dim=3)\n",
    "                grid = torch.cat([grid[0], grid[1], grid[2], grid[3], grid[4], grid[5], grid[6], grid[7]], dim=1)\n",
    "                logger.add_image(\"Test Images\", grid, e)\n",
    "\n",
    "from pathlib import Path\n",
    "exp_name = Path(\"runs/autoencoder_experiment\")\n",
    "exp_i = 0\n",
    "while True:\n",
    "    exp_name_ = exp_name / f\"exp{exp_i}\"\n",
    "    if not exp_name_.exists():\n",
    "        exp_name_.mkdir(parents=True, exist_ok=True)\n",
    "        break\n",
    "    else:\n",
    "        exp_i += 1\n",
    "\n",
    "epochs = 100\n",
    "logger = SummaryWriter(str(exp_name_))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train(model, dl_train, dl_test, optimizer, ssim_loss, device, epochs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d84b8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.12-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from kagglehub) (25.0)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 KB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 KB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset_normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.2/161.2 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 KB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, pyyaml, idna, charset_normalizer, certifi, requests, kagglehub\n",
      "Successfully installed certifi-2025.8.3 charset_normalizer-3.4.3 idna-3.10 kagglehub-0.3.12 pyyaml-6.0.2 requests-2.32.5 urllib3-2.5.0\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/lucadome/streethazards-train?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8.73G/8.73G [10:26<00:00, 15.0MB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/federico/.cache/kagglehub/datasets/lucadome/streethazards-train/versions/1\n"
     ]
    }
   ],
   "source": [
    "# !pip install kagglehub\n",
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"lucadome/streethazards-train\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98e642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
